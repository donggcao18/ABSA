{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#   WARM-UP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import unicodedata\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   index                                            comment  n_star  \\\n",
      "0      0  M·ªõi mua m√°y n√†y T·∫°i thegioididong th·ªët n·ªët c·∫£m...       5   \n",
      "1      1  Pin k√©m c√≤n l·∫°i mi·ªÖn ch√™ mua 8/3/2019 t√¨nh tr·∫°...       5   \n",
      "2      2  Sao l√∫c g·ªçi ƒëi·ªán tho·∫°i m√†n h√¨nh b·ªã ch·∫•m nh·ªè nh...       3   \n",
      "3      3  M·ªçi ng∆∞·ªùi c·∫≠p nh·∫≠t ph·∫ßn m·ªÅm l·∫°i , n√≥ s·∫Ω b·ªõt t·ªë...       3   \n",
      "4      4  M·ªõi mua S√†i ƒë∆∞·ª£c 1 th√°ng th·∫•y pin r·∫•t tr√¢u, S√†...       5   \n",
      "\n",
      "      date_time                                              label  \n",
      "0  2 tu·∫ßn tr∆∞·ªõc  {CAMERA#Positive};{FEATURES#Positive};{BATTERY...  \n",
      "1    14/09/2019    {BATTERY#Negative};{GENERAL#Positive};{OTHERS};  \n",
      "2    17/08/2020                               {FEATURES#Negative};  \n",
      "3    29/02/2020  {FEATURES#Negative};{BATTERY#Neutral};{GENERAL...  \n",
      "4      4/6/2020  {BATTERY#Positive};{PERFORMANCE#Positive};{SER...  \n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 7786 entries, 0 to 7785\n",
      "Data columns (total 5 columns):\n",
      " #   Column     Non-Null Count  Dtype \n",
      "---  ------     --------------  ----- \n",
      " 0   index      7786 non-null   int64 \n",
      " 1   comment    7786 non-null   object\n",
      " 2   n_star     7786 non-null   int64 \n",
      " 3   date_time  7786 non-null   object\n",
      " 4   label      7786 non-null   object\n",
      "dtypes: int64(2), object(3)\n",
      "memory usage: 304.3+ KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "train = pd.read_csv('train.csv')\n",
    "test = pd.read_csv('test.csv')\n",
    "dev = pd.read_csv('dev.csv')\n",
    "print(train.head())\n",
    "print(train.info())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PREPROCESSING DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Text: c√≥ chuy·ªán j k b·∫°n üëç\n",
      "Processed Text: c√≥ chuy·ªán g√¨ kh√¥ng b·∫°n \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0       m·ªõi mua m√°y n√†y t·∫°i thegioididong th·ªët n·ªët c·∫£m...\n",
       "1       pin k√©m c√≤n l·∫°i mi·ªÖn ch√™ mua 832019 t√¨nh tr·∫°ng...\n",
       "2       sao l√∫c g·ªçi ƒëi·ªán tho·∫°i m√†n h√¨nh b·ªã ch·∫•m nh·ªè nh...\n",
       "3       m·ªçi ng∆∞·ªùi c·∫≠p nh·∫≠t ph·∫ßn m·ªÅm l·∫°i  n√≥ s·∫Ω b·ªõt t·ªën...\n",
       "4       m·ªõi mua s√†i ƒë∆∞·ª£c 1 th√°ng th·∫•y pin r·∫•t tr√¢u s√†i...\n",
       "                              ...                        \n",
       "7781    8g c√°i ƒëi ƒë√°nh l√† m·∫°ng gi·∫≠t gi·∫≠t kh√¥ng ch·ªãu n·ªï...\n",
       "7782    mua dk gi·∫£m 500k m√† l·ªói l√≤i ra h·∫øt treo m√†n h√¨...\n",
       "7783    m√°y s√†i 3 th√°ng r·ªìi r·∫•t okpin tr√¢u kh·ªèi n√≥i s√†...\n",
       "7784    r·∫•t ti·∫øc h√†ng realme kh√¥ng c√≥ ·ªëp l∆∞ng ngo√†i  n...\n",
       "7785    m√¨nh r·∫•t th·∫•t v·ªçng khi mua m√°y n√†y b·∫Øt wifi c·ª±...\n",
       "Name: comment, Length: 7786, dtype: object"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# preprocess comment\n",
    "def basic(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^\\w\\s!?]', '', text)\n",
    "    return text\n",
    "\n",
    "slang_dict = {\n",
    "    'ok': '·ªïn',\n",
    "    'oke': '·ªïn',\n",
    "    'oki': '·ªïn',\n",
    "    'okay': '·ªïn',\n",
    "    'k': 'kh√¥ng',\n",
    "    'ko': 'kh√¥ng',\n",
    "    'j': 'g√¨',\n",
    "    'ƒëc': 'ƒë∆∞·ª£c'\n",
    "}\n",
    "\n",
    "def handle_slang(text, slang_dict=slang_dict):\n",
    "    for slang, formal in slang_dict.items():\n",
    "        text = re.sub(r'\\b' + slang + r'\\b', formal, text)\n",
    "    return text\n",
    "\n",
    "def handle_emoji(text):\n",
    "    return ''.join(char for char in text if not unicodedata.category(char).startswith('So'))\n",
    "\n",
    "def preprocess_comment(text):\n",
    "    # Convert to Unicode NFC format\n",
    "    text = unicodedata.normalize('NFC', text)\n",
    "    text = basic(text)\n",
    "    text = handle_slang(text)\n",
    "    text = handle_emoji(text)\n",
    "    return text\n",
    "\n",
    "text = \"c√≥ chuy·ªán j k b·∫°n üëç\"\n",
    "processed_text = preprocess_comment(text)\n",
    "print(\"Original Text:\", text)\n",
    "print(\"Processed Text:\", processed_text)\n",
    "\n",
    "dev['comment'].apply(preprocess_comment)\n",
    "train['comment'].apply(preprocess_comment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{CAMERA#Positive};{PERFORMANCE#Negative};{GENERAL#Neutral}\n"
     ]
    }
   ],
   "source": [
    "def preprocess_label (df):\n",
    "    columns_to_drop = ['n_star', 'date_time']\n",
    "    df.drop(columns=[col for col in columns_to_drop if col in df.columns], inplace=True)\n",
    "    df['label'] = df['label'].str.replace(r';?\\{OTHERS\\};?', '', regex=True).str.strip(';')\n",
    "    return df\n",
    "preprocess_label(dev)\n",
    "print(dev['label'].iloc[105])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    [tensor(1.), tensor(0.), tensor(0.), tensor(1....\n",
      "1    [tensor(0.), tensor(1.), tensor(0.), tensor(0....\n",
      "2    [tensor(0.), tensor(0.), tensor(0.), tensor(0....\n",
      "Name: label, dtype: object\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# preprocess label method 1\n",
    "#turn label into tensor\n",
    "#(issues) there are some comments onnly having OTHERS label\n",
    "\n",
    "aspect_categories = ['BATTERY', 'CAMERA', 'DESIGN', 'FEATURES', 'GENERAL', 'PERFORMANCE', 'PRICE', 'SCREEN', 'SER&ACC', 'STORAGE']    \n",
    "polarity_to_onehot = { 'Positive': [1,0,0], 'Negative': [0,1,0], 'Neutral': [0,0,1]} \n",
    "\n",
    "def label_to_tensor(label: str, aspect_categories: list, polarity_to_onehot: dict):\n",
    "    tensor = torch.zeros((len(aspect_categories), len(polarity_to_onehot)))\n",
    "    components = label.split(';')\n",
    "    for component in components:\n",
    "        component = component.strip('{}')\n",
    "        if '#' in component:\n",
    "            aspect, polarity = component.split('#')\n",
    "            if aspect in aspect_categories:\n",
    "                aspect_idx = aspect_categories.index(aspect)\n",
    "                tensor[aspect_idx] = torch.tensor(polarity_to_onehot[polarity], dtype=torch.float32)\n",
    "\n",
    "                \n",
    "    return tensor.flatten()\n",
    "\n",
    "df = pd.DataFrame({'label': [\"{CAMERA#Positive};{BATTERY#Positive};{FEATURES#Negative}\", \n",
    "                             \"{CAMERA#Neutral};{BATTERY#Negative}\", \"{OTHERS}\"]})  \n",
    "df['label'] = df['label'].apply(lambda x: label_to_tensor(x, aspect_categories, polarity_to_onehot))\n",
    "print (df['label'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>comment</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>ƒëi·ªán tho·∫£i ·ªïn facelock c·ª±c nhanh v√¢n tay √¥k  m...</td>\n",
       "      <td>[tensor(0.), tensor(0.), tensor(0.), tensor(0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>m√¨nh m·ªõi  mua vivo91c t·∫£i ·ª©ng d·ª•ng games  nhan...</td>\n",
       "      <td>[tensor(0.), tensor(0.), tensor(0.), tensor(0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>x·∫•u ƒë·∫πp g√¨ kh√¥ng bi·∫øt nh∆∞ng r·∫•t ∆∞ng tgdƒë ph·ª•c ...</td>\n",
       "      <td>[tensor(0.), tensor(0.), tensor(0.), tensor(0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>m√†n h√¨nh h∆°i l√°c khi ch∆°i game game n·∫∑ng th√¨ m...</td>\n",
       "      <td>[tensor(0.), tensor(0.), tensor(0.), tensor(0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>n√≥i chung m√°y ƒë·∫πp v·ªõi m√†n amoled ·ªïn trong t·∫ßm ...</td>\n",
       "      <td>[tensor(0.), tensor(1.), tensor(0.), tensor(0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2219</th>\n",
       "      <td>2219</td>\n",
       "      <td>m·∫´u m√£ ƒë·∫πp lung linh m√°y ch·∫°y c·ª±c nhanh m∆∞·ª£t h...</td>\n",
       "      <td>[tensor(0.), tensor(0.), tensor(0.), tensor(0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2220</th>\n",
       "      <td>2220</td>\n",
       "      <td>c√≥ ai b·ªã gi·ªëng m√¨nh kh√¥ng m√°y th√¨ s√†i b√¨nh th∆∞...</td>\n",
       "      <td>[tensor(0.), tensor(0.), tensor(0.), tensor(0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2221</th>\n",
       "      <td>2221</td>\n",
       "      <td>s·∫£n ph·∫©m t·ªët \\nai ch∆°i game c·ª© mang 1 em v·ªÅ m√†...</td>\n",
       "      <td>[tensor(0.), tensor(0.), tensor(0.), tensor(0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2222</th>\n",
       "      <td>2222</td>\n",
       "      <td>v·ª´a m·ªõi mua xong m√°y r·∫•t ƒë·∫πp nh√¢n vi√™n r·∫•t nhi...</td>\n",
       "      <td>[tensor(1.), tensor(0.), tensor(0.), tensor(1....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2223</th>\n",
       "      <td>2223</td>\n",
       "      <td>m√¨nh mua dt c≈© nh∆∞ng x√†i r·∫•t t·ªët pin xu·ªëng l√¢u...</td>\n",
       "      <td>[tensor(1.), tensor(0.), tensor(0.), tensor(0....</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2224 rows √ó 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      index                                            comment  \\\n",
       "0         0  ƒëi·ªán tho·∫£i ·ªïn facelock c·ª±c nhanh v√¢n tay √¥k  m...   \n",
       "1         1  m√¨nh m·ªõi  mua vivo91c t·∫£i ·ª©ng d·ª•ng games  nhan...   \n",
       "2         2  x·∫•u ƒë·∫πp g√¨ kh√¥ng bi·∫øt nh∆∞ng r·∫•t ∆∞ng tgdƒë ph·ª•c ...   \n",
       "3         3  m√†n h√¨nh h∆°i l√°c khi ch∆°i game game n·∫∑ng th√¨ m...   \n",
       "4         4  n√≥i chung m√°y ƒë·∫πp v·ªõi m√†n amoled ·ªïn trong t·∫ßm ...   \n",
       "...     ...                                                ...   \n",
       "2219   2219  m·∫´u m√£ ƒë·∫πp lung linh m√°y ch·∫°y c·ª±c nhanh m∆∞·ª£t h...   \n",
       "2220   2220  c√≥ ai b·ªã gi·ªëng m√¨nh kh√¥ng m√°y th√¨ s√†i b√¨nh th∆∞...   \n",
       "2221   2221  s·∫£n ph·∫©m t·ªët \\nai ch∆°i game c·ª© mang 1 em v·ªÅ m√†...   \n",
       "2222   2222  v·ª´a m·ªõi mua xong m√°y r·∫•t ƒë·∫πp nh√¢n vi√™n r·∫•t nhi...   \n",
       "2223   2223  m√¨nh mua dt c≈© nh∆∞ng x√†i r·∫•t t·ªët pin xu·ªëng l√¢u...   \n",
       "\n",
       "                                                  label  \n",
       "0     [tensor(0.), tensor(0.), tensor(0.), tensor(0....  \n",
       "1     [tensor(0.), tensor(0.), tensor(0.), tensor(0....  \n",
       "2     [tensor(0.), tensor(0.), tensor(0.), tensor(0....  \n",
       "3     [tensor(0.), tensor(0.), tensor(0.), tensor(0....  \n",
       "4     [tensor(0.), tensor(1.), tensor(0.), tensor(0....  \n",
       "...                                                 ...  \n",
       "2219  [tensor(0.), tensor(0.), tensor(0.), tensor(0....  \n",
       "2220  [tensor(0.), tensor(0.), tensor(0.), tensor(0....  \n",
       "2221  [tensor(0.), tensor(0.), tensor(0.), tensor(0....  \n",
       "2222  [tensor(1.), tensor(0.), tensor(0.), tensor(1....  \n",
       "2223  [tensor(1.), tensor(0.), tensor(0.), tensor(0....  \n",
       "\n",
       "[2224 rows x 3 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "def final_preprocess_v1(table):\n",
    "    table['comment'] = table['comment'].apply(preprocess_comment)\n",
    "    preprocess_label(table)\n",
    "    table['label'] = table['label'].apply(lambda x: label_to_tensor(x, aspect_categories, polarity_to_onehot))\n",
    "    return table\n",
    "final_preprocess_v1(dev)\n",
    "final_preprocess_v1(train)\n",
    "final_preprocess_v1(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>comment</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>ƒëi·ªán tho·∫£i ·ªïn facelock c·ª±c nhanh v√¢n tay √¥k  m...</td>\n",
       "      <td>[tensor(1.), tensor(0.), tensor(0.), tensor(0.)]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>m√¨nh m·ªõi  mua vivo91c t·∫£i ·ª©ng d·ª•ng games  nhan...</td>\n",
       "      <td>[tensor(1.), tensor(0.), tensor(0.), tensor(0.)]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>x·∫•u ƒë·∫πp g√¨ kh√¥ng bi·∫øt nh∆∞ng r·∫•t ∆∞ng tgdƒë ph·ª•c ...</td>\n",
       "      <td>[tensor(0.), tensor(0.), tensor(0.), tensor(1.)]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>m√†n h√¨nh h∆°i l√°c khi ch∆°i game game n·∫∑ng th√¨ m...</td>\n",
       "      <td>[tensor(0.), tensor(1.), tensor(0.), tensor(0.)]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>n√≥i chung m√°y ƒë·∫πp v·ªõi m√†n amoled ·ªïn trong t·∫ßm ...</td>\n",
       "      <td>[tensor(0.), tensor(0.), tensor(0.), tensor(1.)]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2219</th>\n",
       "      <td>2219</td>\n",
       "      <td>m·∫´u m√£ ƒë·∫πp lung linh m√°y ch·∫°y c·ª±c nhanh m∆∞·ª£t h...</td>\n",
       "      <td>[tensor(1.), tensor(0.), tensor(0.), tensor(0.)]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2220</th>\n",
       "      <td>2220</td>\n",
       "      <td>c√≥ ai b·ªã gi·ªëng m√¨nh kh√¥ng m√°y th√¨ s√†i b√¨nh th∆∞...</td>\n",
       "      <td>[tensor(0.), tensor(1.), tensor(0.), tensor(0.)]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2221</th>\n",
       "      <td>2221</td>\n",
       "      <td>s·∫£n ph·∫©m t·ªët \\nai ch∆°i game c·ª© mang 1 em v·ªÅ m√†...</td>\n",
       "      <td>[tensor(1.), tensor(0.), tensor(0.), tensor(0.)]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2222</th>\n",
       "      <td>2222</td>\n",
       "      <td>v·ª´a m·ªõi mua xong m√°y r·∫•t ƒë·∫πp nh√¢n vi√™n r·∫•t nhi...</td>\n",
       "      <td>[tensor(1.), tensor(0.), tensor(0.), tensor(0.)]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2223</th>\n",
       "      <td>2223</td>\n",
       "      <td>m√¨nh mua dt c≈© nh∆∞ng x√†i r·∫•t t·ªët pin xu·ªëng l√¢u...</td>\n",
       "      <td>[tensor(0.), tensor(0.), tensor(0.), tensor(1.)]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2224 rows √ó 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      index                                            comment  \\\n",
       "0         0  ƒëi·ªán tho·∫£i ·ªïn facelock c·ª±c nhanh v√¢n tay √¥k  m...   \n",
       "1         1  m√¨nh m·ªõi  mua vivo91c t·∫£i ·ª©ng d·ª•ng games  nhan...   \n",
       "2         2  x·∫•u ƒë·∫πp g√¨ kh√¥ng bi·∫øt nh∆∞ng r·∫•t ∆∞ng tgdƒë ph·ª•c ...   \n",
       "3         3  m√†n h√¨nh h∆°i l√°c khi ch∆°i game game n·∫∑ng th√¨ m...   \n",
       "4         4  n√≥i chung m√°y ƒë·∫πp v·ªõi m√†n amoled ·ªïn trong t·∫ßm ...   \n",
       "...     ...                                                ...   \n",
       "2219   2219  m·∫´u m√£ ƒë·∫πp lung linh m√°y ch·∫°y c·ª±c nhanh m∆∞·ª£t h...   \n",
       "2220   2220  c√≥ ai b·ªã gi·ªëng m√¨nh kh√¥ng m√°y th√¨ s√†i b√¨nh th∆∞...   \n",
       "2221   2221  s·∫£n ph·∫©m t·ªët \\nai ch∆°i game c·ª© mang 1 em v·ªÅ m√†...   \n",
       "2222   2222  v·ª´a m·ªõi mua xong m√°y r·∫•t ƒë·∫πp nh√¢n vi√™n r·∫•t nhi...   \n",
       "2223   2223  m√¨nh mua dt c≈© nh∆∞ng x√†i r·∫•t t·ªët pin xu·ªëng l√¢u...   \n",
       "\n",
       "                                                 label  \n",
       "0     [tensor(1.), tensor(0.), tensor(0.), tensor(0.)]  \n",
       "1     [tensor(1.), tensor(0.), tensor(0.), tensor(0.)]  \n",
       "2     [tensor(0.), tensor(0.), tensor(0.), tensor(1.)]  \n",
       "3     [tensor(0.), tensor(1.), tensor(0.), tensor(0.)]  \n",
       "4     [tensor(0.), tensor(0.), tensor(0.), tensor(1.)]  \n",
       "...                                                ...  \n",
       "2219  [tensor(1.), tensor(0.), tensor(0.), tensor(0.)]  \n",
       "2220  [tensor(0.), tensor(1.), tensor(0.), tensor(0.)]  \n",
       "2221  [tensor(1.), tensor(0.), tensor(0.), tensor(0.)]  \n",
       "2222  [tensor(1.), tensor(0.), tensor(0.), tensor(0.)]  \n",
       "2223  [tensor(0.), tensor(0.), tensor(0.), tensor(1.)]  \n",
       "\n",
       "[2224 rows x 3 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define a function to filter for \"PERFORMANCE\" aspect\n",
    "def filter_performance_only(label):\n",
    "    # Retain only the entries with \"PERFORMANCE\"\n",
    "    performance_only = ';'.join([item for item in label.split(';') if item.startswith(\"{PERFORMANCE\")])\n",
    "    return performance_only\n",
    "\n",
    "def polarity_to_one_hot(label):\n",
    "    if \"PERFORMANCE#Positive\" in label:\n",
    "        return torch.tensor([1, 0, 0,0], dtype=torch.float32)\n",
    "    elif \"PERFORMANCE#Negative\" in label:\n",
    "        return torch.tensor([0, 1, 0,0], dtype=torch.float32)\n",
    "    elif \"PERFORMANCE#Neutral\" in label:\n",
    "        return torch.tensor([0, 0, 1, 0], dtype=torch.float32)  \n",
    "    else:\n",
    "        return torch.tensor([0, 0, 0, 1], dtype=torch.float32)\n",
    "\n",
    "\n",
    "def final_preprocess(table):\n",
    "    table['comment'] = table['comment'].apply(preprocess_comment)\n",
    "    preprocess_label(table)\n",
    "    table['label'] = table['label'].apply(filter_performance_only)\n",
    "    table['label'] = table['label'].apply(polarity_to_one_hot)\n",
    "    return table\n",
    "\n",
    "final_preprocess(dev)\n",
    "final_preprocess(train)\n",
    "final_preprocess(test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenization and Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ADMIN\\anaconda3\\Lib\\site-packages\\torchtext\\vocab.py:432: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.itos, self.stoi, self.vectors, self.dim = torch.load(path_pt)\n",
      "C:\\Users\\ADMIN\\AppData\\Local\\Temp\\ipykernel_27568\\3274337963.py:28: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  vector=torch.tensor(fastText[token], dtype=torch.float32)\n"
     ]
    }
   ],
   "source": [
    "from torchtext.vocab import FastText\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bkai-foundation-models/vietnamese-bi-encoder\")\n",
    "fastText = FastText(language='vi')\n",
    "\n",
    "MAX_LENGTH = 128\n",
    "BATCH_SIZE = 32\n",
    "EMBEDDING_DIM = fastText.dim  \n",
    "VOCAB_SIZE = tokenizer.vocab_size\n",
    "\n",
    "encoding = tokenizer(\n",
    "    train['comment'].tolist(),\n",
    "    padding=\"max_length\",\n",
    "    truncation=True,\n",
    "    max_length=MAX_LENGTH,\n",
    "    return_tensors=\"pt\"\n",
    ")\n",
    "token_ids = encoding['input_ids']\n",
    "tokens = []\n",
    "\n",
    "for each in token_ids:\n",
    "    temp = tokenizer.convert_ids_to_tokens(each)\n",
    "    tokens.append(temp)\n",
    "\n",
    "embedding_matrix = torch.zeros(VOCAB_SIZE, EMBEDDING_DIM)\n",
    "for each in tokens:\n",
    "    for token in each:\n",
    "        if token in fastText.stoi:\n",
    "            vector=torch.tensor(fastText[token], dtype=torch.float32)\n",
    "        else:\n",
    "            vector = torch.zeros(EMBEDDING_DIM)\n",
    "        embedding_matrix[tokenizer.convert_tokens_to_ids(token)] = vector\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(df):\n",
    "    texts = df[\"comment\"].tolist()\n",
    "    labels = df[\"label\"].tolist()\n",
    "\n",
    "    encoding = tokenizer(\n",
    "        texts,\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        max_length=MAX_LENGTH,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    token_ids = encoding['input_ids'] \n",
    "    return token_ids, labels\n",
    "\n",
    "\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, token_ids, labels):\n",
    "        self.token_ids = token_ids\n",
    "        self.labels = labels\n",
    "    def __len__(self):\n",
    "        return len(self.token_ids)\n",
    "    def __getitem__(self, idx):\n",
    "        tokens = self.token_ids[idx]\n",
    "        label = torch.tensor(self.labels[idx], dtype=torch.float32)  \n",
    "        return tokens, label\n",
    "\n",
    "def create_dataloader(df, batch_size=BATCH_SIZE):\n",
    "    token_ids, labels = tokenize(df)\n",
    "    dataset = TextDataset(token_ids, labels)\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "    return dataloader\n",
    "\n",
    "\n",
    "\n",
    "train_loader = create_dataloader(train)\n",
    "dev_loader = create_dataloader(dev)\n",
    "test_loader = create_dataloader(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 128])\n",
      "torch.Size([32, 4])\n",
      "244\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ADMIN\\AppData\\Local\\Temp\\ipykernel_27568\\3896083272.py:24: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  label = torch.tensor(self.labels[idx], dtype=torch.float32)\n"
     ]
    }
   ],
   "source": [
    "for batch in train_loader:\n",
    "    tokens, labels = batch\n",
    "    print(tokens.shape)\n",
    "    print(labels.shape)\n",
    "    break\n",
    "print (len(train_loader))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model_v1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ABSA model from SA2SL paper\n",
    "class ABSA(nn.Module):\n",
    "    def __init__(self, embedd_matrix=embedding_matrix, EMBED_DIM=EMBEDDING_DIM, LSTM_UNITS=128, conv_filters=64):\n",
    "        super(ABSA, self).__init__()\n",
    "        self.embedding = nn.Embedding.from_pretrained(embedd_matrix, \n",
    "                                                      freeze=True)\n",
    "        self.spatial_dropout = nn.Dropout2d(0.35)\n",
    "        self.lstm = nn.LSTM(EMBED_DIM, \n",
    "                            LSTM_UNITS, \n",
    "                            bidirectional=True, \n",
    "                            batch_first=True, \n",
    "                            dropout=0.15)\n",
    "        \n",
    "        self.conv1d = nn.Conv1d(LSTM_UNITS * 2, \n",
    "                                conv_filters, \n",
    "                                kernel_size=3, \n",
    "                                padding='valid')\n",
    "        self.avg_pool = nn.AdaptiveAvgPool1d(1)\n",
    "        self.max_pool = nn.AdaptiveMaxPool1d(1)\n",
    "        self.fc = nn.Linear(conv_filters * 2, 30) \n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        x = x.permute(0, 2, 1)  #(batch, channels, seq_len) for dropout2d\n",
    "        x = self.spatial_dropout(x)\n",
    "        x = x.permute(0, 2, 1)  # back to (batch, seq_len, channels)\n",
    "        x, _ = self.lstm(x)\n",
    "        x = x.permute(0, 2, 1)  # (batch, channels, seq_len)\n",
    "        x = self.conv1d(x)\n",
    "        avg_pool = self.avg_pool(x).squeeze(-1)\n",
    "        max_pool = self.max_pool(x).squeeze(-1)\n",
    "        x = torch.cat((avg_pool, max_pool), dim=1)\n",
    "        out = torch.sigmoid(self.fc(x)) \n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ADMIN\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.15 and num_layers=1\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BCELoss()"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = ABSA()\n",
    "criterion = nn.BCELoss()  \n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "model.to(device)\n",
    "criterion.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ADMIN\\AppData\\Local\\Temp\\ipykernel_5608\\3896083272.py:24: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  label = torch.tensor(self.labels[idx], dtype=torch.float32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20, Training Loss: 0.0401, Training Acc: 0.40701844262295084\n",
      "Validation Loss: 0.3060, Validation Acc: 0.4005952380952381\n",
      "Epoch 2/20, Training Loss: 0.0375, Training Acc: 0.4106301229508197\n",
      "Validation Loss: 0.3021, Validation Acc: 0.40520833333333334\n",
      "Epoch 3/20, Training Loss: 0.0368, Training Acc: 0.43033213797814207\n",
      "Validation Loss: 0.3085, Validation Acc: 0.40633928571428574\n",
      "Epoch 4/20, Training Loss: 0.0393, Training Acc: 0.4385715505464481\n",
      "Validation Loss: 0.3077, Validation Acc: 0.44026785714285716\n",
      "Epoch 5/20, Training Loss: 0.0384, Training Acc: 0.45703125\n",
      "Validation Loss: 0.3333, Validation Acc: 0.44300595238095236\n",
      "Epoch 6/20, Training Loss: 0.0361, Training Acc: 0.4721311475409836\n",
      "Validation Loss: 0.3355, Validation Acc: 0.44660714285714287\n",
      "Epoch 7/20, Training Loss: 0.0352, Training Acc: 0.4774248633879781\n",
      "Validation Loss: 0.3366, Validation Acc: 0.4855952380952381\n",
      "Epoch 8/20, Training Loss: 0.3260, Training Acc: 0.5591359289617487\n",
      "Validation Loss: 0.5613, Validation Acc: 0.5844940476190477\n",
      "Epoch 9/20, Training Loss: 0.1680, Training Acc: 0.565620730874317\n",
      "Validation Loss: 0.3885, Validation Acc: 0.5608630952380952\n",
      "Epoch 10/20, Training Loss: 0.1254, Training Acc: 0.512286543715847\n",
      "Validation Loss: 0.3696, Validation Acc: 0.48291666666666666\n",
      "Epoch 11/20, Training Loss: 0.1063, Training Acc: 0.48155310792349726\n",
      "Validation Loss: 0.3324, Validation Acc: 0.4969345238095238\n",
      "Epoch 12/20, Training Loss: 0.0968, Training Acc: 0.4663763661202186\n",
      "Validation Loss: 0.3259, Validation Acc: 0.46226190476190476\n",
      "Epoch 13/20, Training Loss: 0.0918, Training Acc: 0.460864924863388\n",
      "Validation Loss: 0.3262, Validation Acc: 0.4706845238095238\n",
      "Epoch 14/20, Training Loss: 0.0871, Training Acc: 0.45610911885245903\n",
      "Validation Loss: 0.3204, Validation Acc: 0.4475892857142857\n",
      "Epoch 15/20, Training Loss: 0.0830, Training Acc: 0.44776724726775957\n",
      "Validation Loss: 0.3038, Validation Acc: 0.4423809523809524\n",
      "Epoch 16/20, Training Loss: 0.0813, Training Acc: 0.4511057035519126\n",
      "Validation Loss: 0.3190, Validation Acc: 0.42791666666666667\n",
      "Epoch 17/20, Training Loss: 0.0763, Training Acc: 0.4476904030054645\n",
      "Validation Loss: 0.3455, Validation Acc: 0.3994345238095238\n",
      "Epoch 18/20, Training Loss: 0.0762, Training Acc: 0.4452356557377049\n",
      "Validation Loss: 0.3292, Validation Acc: 0.4585714285714286\n",
      "Epoch 19/20, Training Loss: 0.0763, Training Acc: 0.4471695696721312\n",
      "Validation Loss: 0.3298, Validation Acc: 0.4452380952380952\n",
      "Epoch 20/20, Training Loss: 0.0770, Training Acc: 0.45652322404371587\n",
      "Validation Loss: 0.3271, Validation Acc: 0.47717261904761904\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 20\n",
    "BATCH_SIZE = 32 \n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    model.train()\n",
    "    correct_predictions = 0\n",
    "    total_loss = 0\n",
    "    \n",
    "    for i, batch in enumerate(train_loader):\n",
    "        tokens, labels = batch\n",
    "        tokens, labels = tokens.to(device), labels.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        output = model(tokens)\n",
    "        loss = criterion(output, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        predictions = torch.sigmoid(output) > 0.5\n",
    "        correct_predictions += (predictions == labels).sum().item()\n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    avg_acc = correct_predictions / (len(train_loader) * BATCH_SIZE*30)\n",
    "    print(f\"Epoch {epoch+1}/{EPOCHS}, Training Loss: {avg_loss:.4f}, Training Acc: {avg_acc}\")\n",
    "    \n",
    "    model.eval()\n",
    "    correct_predictions = 0\n",
    "    total_loss = 0\n",
    "    \n",
    "    with torch.inference_mode():\n",
    "        for i, batch in enumerate(dev_loader):\n",
    "            tokens, labels = batch\n",
    "            tokens, labels = tokens.to(device), labels.to(device)\n",
    "            output = model(tokens)\n",
    "            loss = criterion(output, labels)\n",
    "            predictions = torch.sigmoid(output) > 0.5\n",
    "            correct_predictions += (predictions == labels).sum().item()\n",
    "            total_loss += loss.item()\n",
    "        \n",
    "        avg_val_loss = total_loss / len(dev_loader)\n",
    "        avg_val_acc = correct_predictions / (len(dev_loader) * BATCH_SIZE*30)\n",
    "        print(f\"Validation Loss: {avg_val_loss:.4f}, Validation Acc: {avg_val_acc}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model_v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ABSA v2 with attention-based layer\n",
    "import math\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, embed_dim=EMBEDDING_DIM, hidden_dim=None, out_dim=None, n_head=1, score_function='dot_product', dropout=0):\n",
    "        ''' Attention Mechanism '''\n",
    "        super(Attention, self).__init__()\n",
    "        if hidden_dim is None:\n",
    "            hidden_dim = embed_dim // n_head\n",
    "        if out_dim is None:\n",
    "            out_dim = embed_dim\n",
    "        self.embed_dim = embed_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.n_head = n_head\n",
    "        self.score_function = score_function\n",
    "        self.w_k = nn.Linear(embed_dim, n_head * hidden_dim)\n",
    "        self.w_q = nn.Linear(embed_dim, n_head * hidden_dim)\n",
    "        self.proj = nn.Linear(n_head * hidden_dim, out_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        if score_function == 'mlp':\n",
    "            self.weight = nn.Parameter(torch.Tensor(hidden_dim*2))\n",
    "        if score_function == 'nl':\n",
    "            self.weight = nn.parameter(torch.Tensor(hidden_dim, hidden_dim))\n",
    "        elif self.score_function == 'bi_linear':\n",
    "            self.weight = nn.Parameter(torch.Tensor(hidden_dim, hidden_dim))\n",
    "        else:  # dot_product / scaled_dot_product\n",
    "            self.register_parameter('weight', None)\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        stdv = 1. / math.sqrt(self.hidden_dim)\n",
    "        if self.weight is not None:\n",
    "            self.weight.data.uniform_(-stdv, stdv)\n",
    "\n",
    "    def forward(self, k, q):\n",
    "        if len(q.shape) == 2: \n",
    "            q = torch.unsqueeze(q, dim=1)\n",
    "        if len(k.shape) == 2:  \n",
    "            k = torch.unsqueeze(k, dim=1)\n",
    "        mb_size = k.shape[0]  \n",
    "        k_len = k.shape[1]\n",
    "        q_len = q.shape[1]\n",
    "        kx = self.w_k(k).view(mb_size, k_len, self.n_head, self.hidden_dim)\n",
    "        kx = kx.permute(2, 0, 1, 3).contiguous().view(-1, k_len, self.hidden_dim)\n",
    "        qx = self.w_q(q).view(mb_size, q_len, self.n_head, self.hidden_dim)\n",
    "        qx = qx.permute(2, 0, 1, 3).contiguous().view(-1, q_len, self.hidden_dim)\n",
    "        if self.score_function == 'dot_product':\n",
    "            kt = kx.permute(0, 2, 1)\n",
    "            score = torch.bmm(qx, kt)\n",
    "        elif self.score_function == 'scaled_dot_product':\n",
    "            kt = kx.permute(0, 2, 1)\n",
    "            qkt = torch.bmm(qx, kt)\n",
    "            score = torch.div(qkt, math.sqrt(self.hidden_dim))\n",
    "        elif self.score_function == 'bi_linear':\n",
    "            qw = torch.matmul(qx, self.weight)\n",
    "            kt = kx.permute(0, 2, 1)\n",
    "            score = torch.bmm(qw, kt)\n",
    "        else:\n",
    "            raise RuntimeError('invalid score_function')\n",
    "        score = F.softmax(score, dim=-1)\n",
    "        # in sentiment analysis, they focus to the importance of k, so maybe we dont have V  value (intuitively, V is k...)\n",
    "        output = torch.bmm(score, kx) \n",
    "        output = torch.cat(torch.split(output, mb_size, dim=0), dim=-1)  \n",
    "        output = self.proj(output)  \n",
    "        output = self.dropout(output)\n",
    "        return output, score\n",
    "\n",
    "\n",
    "class NoQueryAttention(Attention):\n",
    "    '''q is a parameter'''\n",
    "    def __init__(self, embed_dim=EMBEDDING_DIM, hidden_dim=None, out_dim=None, n_head=1, score_function='dot_product', q_len=1, dropout=0):\n",
    "        super(NoQueryAttention, self).__init__(embed_dim, hidden_dim, out_dim, n_head, score_function, dropout)\n",
    "        self.q_len = q_len\n",
    "        self.q = nn.Parameter(torch.Tensor(q_len, embed_dim))\n",
    "        self.reset_q()\n",
    "\n",
    "    def reset_q(self):\n",
    "        stdv = 1. / math.sqrt(self.embed_dim)\n",
    "        self.q.data.uniform_(-stdv, stdv)\n",
    "\n",
    "    def forward(self, k, **kwargs):\n",
    "        mb_size = k.shape[0]\n",
    "        q = self.q.expand(mb_size, -1, -1)\n",
    "        return super(NoQueryAttention, self).forward(k, q)\n",
    "\n",
    "class ATAE_LSTM(nn.Module):\n",
    "    def __init__(self, embed_matrix=embedding_matrix, hidden_dim=128, embedding_dim=EMBEDDING_DIM, polarities_dim=4):\n",
    "        super(ATAE_LSTM, self).__init__()\n",
    "        self.embed = nn.Embedding.from_pretrained(torch.tensor(embed_matrix, dtype=torch.float))\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, num_layers=1, batch_first=True)\n",
    "        self.attention = NoQueryAttention(hidden_dim, score_function='bi_linear')\n",
    "        self.dense = nn.Linear(hidden_dim, polarities_dim)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, text_indices):\n",
    "        x = self.embed(text_indices)\n",
    "        h, _ = self.lstm(x)\n",
    "        _, score = self.attention(h)\n",
    "        output = torch.bmm(score, h).squeeze(dim=1)  # Squeeze to (batch_size, hidden_dim)\n",
    "        out = self.dense(output)\n",
    "        out = self.sigmoid(out)\n",
    "        return out\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ADMIN\\AppData\\Local\\Temp\\ipykernel_27568\\2743902255.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.embed = nn.Embedding.from_pretrained(torch.tensor(embed_matrix, dtype=torch.float))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ADMIN\\AppData\\Local\\Temp\\ipykernel_27568\\3896083272.py:24: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  label = torch.tensor(self.labels[idx], dtype=torch.float32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20, Loss: 0.3997, Accuracy: 0.5491\n",
      "Epoch 2/20, Loss: 0.3719, Accuracy: 0.6237\n",
      "Epoch 3/20, Loss: 0.3774, Accuracy: 0.6187\n",
      "Epoch 4/20, Loss: 0.3707, Accuracy: 0.6327\n",
      "Epoch 5/20, Loss: 0.4077, Accuracy: 0.5565\n",
      "Epoch 6/20, Loss: 0.4354, Accuracy: 0.4848\n",
      "Epoch 7/20, Loss: 0.4045, Accuracy: 0.5581\n",
      "Epoch 8/20, Loss: 0.3882, Accuracy: 0.5831\n",
      "Epoch 9/20, Loss: 0.3675, Accuracy: 0.6120\n",
      "Epoch 10/20, Loss: 0.3428, Accuracy: 0.6543\n",
      "Epoch 11/20, Loss: 0.3604, Accuracy: 0.6134\n",
      "Epoch 12/20, Loss: 0.3462, Accuracy: 0.6531\n",
      "Epoch 13/20, Loss: 0.3260, Accuracy: 0.6853\n",
      "Epoch 14/20, Loss: 0.3785, Accuracy: 0.6012\n",
      "Epoch 15/20, Loss: 0.3869, Accuracy: 0.6058\n",
      "Epoch 16/20, Loss: 0.3800, Accuracy: 0.6078\n",
      "Epoch 17/20, Loss: 0.3607, Accuracy: 0.6315\n",
      "Epoch 18/20, Loss: 0.3585, Accuracy: 0.6446\n",
      "Epoch 19/20, Loss: 0.3419, Accuracy: 0.6458\n",
      "Epoch 20/20, Loss: 0.3479, Accuracy: 0.6507\n"
     ]
    }
   ],
   "source": [
    "model_v2 = ATAE_LSTM()\n",
    "criterion_v2 = nn.BCELoss() \n",
    "optimizer_v2 = torch.optim.Adam(model_v2.parameters(), lr=0.01)\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(torch.cuda.is_available())\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "model_v2.to(device)\n",
    "criterion_v2.to(device)\n",
    "\n",
    "EPOCHS = 20\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    correct_predictions = 0\n",
    "    total_loss = 0\n",
    "    \n",
    "    for i, batch in enumerate(train_loader):\n",
    "        tokens, labels = batch\n",
    "        tokens, labels = tokens.to(device), labels.to(device)\n",
    "\n",
    "        optimizer_v2.zero_grad()\n",
    "        output = model_v2(tokens)\n",
    "        loss_v2 = criterion_v2(output, labels)\n",
    "        \n",
    "        loss_v2.backward()\n",
    "        optimizer_v2.step()\n",
    "        \n",
    "        predictions = output > 0.5\n",
    "        correct_predictions += (predictions == labels).all(axis=1).sum().item()\n",
    "        total_loss += loss_v2.item()\n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    avg_acc = correct_predictions / len(train_loader.dataset)\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}/{EPOCHS}, Loss: {avg_loss:.4f}, Accuracy: {avg_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model_v3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c0b2ae6cc9904f52833c5b8c481e42a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ADMIN\\anaconda3\\Lib\\site-packages\\huggingface_hub\\file_download.py:159: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\ADMIN\\.cache\\huggingface\\hub\\models--google-bert--bert-base-uncased. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1b141187c9884943a440195c741f57f4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "74ca05e8df8c4eb98c8aa69a62de6310",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "80ef0a1f33ac4e75afb35d4adbd093ba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import BertConfig, BertModel\n",
    "\n",
    "configuration = BertConfig()\n",
    "vocab_size = configuration.vocab_size\n",
    "hidden_size = configuration.hidden_size\n",
    "num_hidden_layers = configuration.num_hidden_layers\n",
    "\n",
    "tokenier = AutoTokenizer.from_pretrained(\"google-bert/bert-base-uncased\")\n",
    "\n",
    "def tokenize_input (sentence):\n",
    "    tokenize = tokenizer (\n",
    "        sentence,\n",
    "        padding = \"max_length\",\n",
    "        truncation = True,\n",
    "        max_length = MAX_LENGTH,\n",
    "        return_tensors = \"pt\"\n",
    "    )\n",
    "    tensor_return = tokenize['input_ids']\n",
    "    return tensor_return\n",
    "\n",
    "class BERT (nn.Module):\n",
    "    def __init__ (self, nclass,embedding_dim, hidden_size, num_hidden_layers):\n",
    "        super(BERT, self).__init__()\n",
    "        self.nclass = nclass\n",
    "        self.tokenizer = tokenier()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_hidden_layers\n",
    "        self.bert = AutoModel.from_pretrained(\"google-bert/bert-base-uncased\",\n",
    "                                              output_hidden_states = True,\n",
    "                                              output_attentions=False)\n",
    "        self.dropout - nn.Dropout(0.3)\n",
    "        self.lstm = torch.nn.LSTM(hidden_size, \n",
    "                                  hidden_size/2, \n",
    "                                  num_layers=1, \n",
    "                                  batch_first=True,\n",
    "                                  bidirectional=True)\n",
    "        self.fc = nn.Linear(hidden_size, nclass)\n",
    "\n",
    "    def foroward (self, input, mask):\n",
    "        input = self.tokenizer(input)\n",
    "        input_ids = input['input_ids']\n",
    "        _, output = self.bert (input_ids=input_ids, \n",
    "                               attention_mask= mask, \n",
    "                               token_type_ids=None)\n",
    "        hidden_states = torch.stack([hidden_states[layer_i][:, 0].squeeze()\n",
    "                                     for layer_i in range(0, self.num_layers)], dim=-1) # noqa\n",
    "        hidden_states = hidden_states.view(-1, self.num_layers, self.hidden_size)\n",
    "        _, output = self.lstm (hidden_states)\n",
    "        output = self.dropout(output)\n",
    "        output = self.fc(output)\n",
    "\n",
    "        return output \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertConfig {\n",
       "  \"attention_probs_dropout_prob\": 0.1,\n",
       "  \"classifier_dropout\": null,\n",
       "  \"hidden_act\": \"gelu\",\n",
       "  \"hidden_dropout_prob\": 0.1,\n",
       "  \"hidden_size\": 768,\n",
       "  \"initializer_range\": 0.02,\n",
       "  \"intermediate_size\": 3072,\n",
       "  \"layer_norm_eps\": 1e-12,\n",
       "  \"max_position_embeddings\": 512,\n",
       "  \"model_type\": \"bert\",\n",
       "  \"num_attention_heads\": 12,\n",
       "  \"num_hidden_layers\": 12,\n",
       "  \"pad_token_id\": 0,\n",
       "  \"position_embedding_type\": \"absolute\",\n",
       "  \"transformers_version\": \"4.46.0\",\n",
       "  \"type_vocab_size\": 2,\n",
       "  \"use_cache\": true,\n",
       "  \"vocab_size\": 30522\n",
       "}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
